# -*- coding: utf-8 -*-
import logging
import time, argparse, os, sys, random
from pathlib import Path
from typing import Tuple, List, Optional, Dict

import numpy as np

from othello import Board
from util import epsilon_greedy
from value import ModelScorer
from ai import Bot

# --------------------------- æ—¥å¿— ---------------------------

def setup_logging(logfile: str, verbose: bool = True):
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    root = logging.getLogger()
    root.handlers.clear()
    root.setLevel(logging.INFO)

    fh = logging.FileHandler(logfile, encoding="utf-8")
    fh.setFormatter(fmt)
    fh.setLevel(logging.INFO)
    root.addHandler(fh)

    if verbose:
        sh = logging.StreamHandler(sys.stdout)
        sh.setFormatter(fmt)
        sh.setLevel(logging.INFO)
        root.addHandler(sh)

# --------------------------- å·¥å…·ï¼šåæ ‡/ç¼–ç /å¯¹ç§° ---------------------------

# å°† (i,j) æ˜ å°„åˆ° 0..63ï¼Œpass=64
def ij_to_a(i: int, j: int) -> int:
    if i < 0 or j < 0:
        return 64
    return i * 8 + j

def a_to_ij(a: int) -> Tuple[int, int]:
    if a == 64:
        return (-1, -1)
    return a // 8, a % 8

# æŠŠ Board ç¼–ç åˆ° (3,8,8) uint8ï¼šchannel0=å½“å‰æ‰§æ‰‹å­ã€channel1=å¯¹æ‰‹å­ã€channel2=å…¨ 1ï¼ˆTurnï¼‰
def board_to_3ch_uint8(b: Board, p: int) -> np.ndarray:
    # Board å†…éƒ¨çº¦å®šï¼šé»‘=1 ç™½=-1ï¼ˆå¸¸è§åšæ³•ï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸åŒå¸¸é‡ï¼›ç”¨ API æ¥æ‹¿
    # è¿™é‡Œç”¨ score/å¯è½å­æ¥ä¾§æ¨ä¸ç¨³å¦¥ï¼Œå› æ­¤ç›´æ¥èµ°æ£‹ç›˜æ ¼å­è¯»å€¼
    # å‡è®¾ b._board[i,j] in {Board.BLACK, Board.WHITE, 0}
    # è‹¥ä½ çš„ Board æ²¡å…¬å¼€ _boardï¼Œå¯å¢åŠ  get_cell(i,j) ç­‰æ¥å£ï¼›æ­¤å¤„æŒ‰å¸¸è§å®ç°è¯»å–
    arr = np.zeros((3, 8, 8), dtype=np.uint8)
    me, opp = p, Board.opponent(p)
    for i in range(8):
        for j in range(8):
            v = b._board[i, j]  # è‹¥ä½ çš„ Board æä¾›åˆ«çš„è®¿é—®æ–¹å¼ï¼Œè¯·æ”¹è¿™é‡Œ
            if v == me:
                arr[0, i, j] = 1
            elif v == opp:
                arr[1, i, j] = 1
    arr[2, :, :] = 1  # turn å¹³é¢ï¼Œè¡¨ç¤ºâ€œä»å½“å‰æ‰§æ‰‹è§†è§’â€
    return arr

# ç”Ÿæˆ 8 ä¸ª D4 å¯¹ç§°ï¼ˆæ—‹è½¬/ç¿»è½¬ï¼‰
def d4_variants(x: np.ndarray) -> List[np.ndarray]:
    # x: (3,8,8)
    xs = []
    base = x
    for k in range(4):
        rot = np.rot90(base, k, axes=(1, 2))
        xs.append(rot)
        xs.append(np.flip(rot, axis=2))  # æ°´å¹³ç¿»è½¬
    return xs

def canonicalize_3ch(x: np.ndarray) -> np.ndarray:
    # è¿”å›å­—å…¸åºæœ€å°çš„å½¢æ€ï¼Œé™ä½é‡å¤
    cand = d4_variants(x)
    keys = [np.packbits(c, axis=None).tobytes() for c in cand]
    idx = int(np.argmin(keys))
    return cand[idx]

def legal_mask_from_board(b: Board, p: int) -> np.ndarray:
    m = np.zeros((64,), dtype=np.uint8)
    options = b.feasible_pos(p)
    for (i, j) in options:
        m[ij_to_a(i, j)] = 1
    return m

def hash64_sa(state_3ch_uint8: np.ndarray, a_idx: int) -> int:
    data = np.packbits(state_3ch_uint8, axis=None).tobytes() + bytes([a_idx & 0xFF])
    h = np.frombuffer(
        memoryview(
            __import__("hashlib").blake2b(data, digest_size=8).digest()
        ),
        dtype=">u8",
    )[0]
    return int(h)

# --------------------------- è¯„æµ‹ï¼ˆvs éšæœºï¼‰ ---------------------------

def evaluate_vs_random(model, games: int = 200, seed: int = 123,
                       depth: int = 3, width: int = 6) -> dict:
    rng = random.Random(seed)
    games = games + (games % 2)

    def _play_single_game(black_is_model: bool) -> int:
        b = Board(); b.init_board(); p = Board.BLACK
        black_bot = Bot(model, depth, width, Board.BLACK)
        white_bot = Bot(model, depth, width, Board.WHITE)
        while not b.is_terminal_state():
            options = b.feasible_pos(p)
            if options:
                if (p == Board.BLACK and black_is_model) or (p == Board.WHITE and not black_is_model):
                    if p == Board.BLACK:
                        _, (a0, a1) = black_bot._play(b)
                    else:
                        _, (a0, a1) = white_bot._play(b)
                    b.flip(a0, a1, p)
                else:
                    i, j = rng.choice(options)
                    b.flip(i, j, p)
            p = Board.opponent(p)
        sb, sw = b.score(Board.BLACK), b.score(Board.WHITE)
        result_black = 1 if sb > sw else (-1 if sb < sw else 0)
        return result_black if black_is_model else -result_black

    wins = losses = draws = 0
    for _ in range(0, games, 2):
        r1 = _play_single_game(True)
        r2 = _play_single_game(False)
        for r in (r1, r2):
            if r > 0: wins += 1
            elif r < 0: losses += 1
            else: draws += 1

    n = wins + losses + draws
    wr = wins / n if n else 0.0

    import math
    z = 1.96
    phat = wr
    denom = 1 + z**2 / n
    centre = (phat + z**2 / (2*n)) / denom
    margin = (z/denom) * math.sqrt((phat*(1-phat)/n) + (z**2/(4*n**2)))
    return {
        "winrate": wr, "wins": wins, "losses": losses, "draws": draws, "games": n,
        "ci_low": max(0.0, centre - margin), "ci_high": min(1.0, centre + margin)
    }

# --------------------------- è‡ªåšå¼ˆè®­ç»ƒ ---------------------------

def self_play(episodes, model, epsilon, log_every, eval_every, ckpt_every,
              ckpt_dir, save_latest, seed, train_depth, train_width, args ,
              stop_winrate=0.95):
    rng = random.Random(seed)
    os.makedirs(ckpt_dir, exist_ok=True)
    latest_path = Path(save_latest)
    latest_path.parent.mkdir(parents=True, exist_ok=True)

    b = Board()
    black_bot = Bot(model, train_depth, train_width, Board.BLACK)
    white_bot = Bot(model, train_depth, train_width, Board.WHITE)

    t0 = time.time()
    stat_w = stat_l = stat_d = 0

    for t in range(1, episodes + 1):
        b.init_board()
        p = Board.BLACK

        while not b.is_terminal_state():
            options = b.feasible_pos(p)
            vals = []
            if len(options) > 0:
                if p == Board.BLACK:
                    gr, (ga0, ga1) = black_bot._play(b)
                else:
                    gr, (ga0, ga1) = white_bot._play(b)
                    gr = -gr

                for i, j in options:
                    with b.flip2(i, j, p):
                        if b.is_terminal_state():
                            vals.append(b.score(Board.BLACK) - b.score(Board.WHITE))
                        else:
                            vals.append(model(b))

                (a0, a1), v = epsilon_greedy(epsilon, options, vals, (ga0, ga1), gr)
                model.update(b, v)
                b.flip(a0, a1, p)

            p = Board.opponent(p)

        sb = b.score(Board.BLACK)
        sw = b.score(Board.WHITE)
        if sb > sw: stat_w += 1
        elif sb < sw: stat_l += 1
        else: stat_d += 1

                # ========== æ–°å¢ï¼šèƒœç‡åœæ­¢æ£€æŸ¥ ==========
        if eval_every > 0 and t % eval_every == 0:
            eval_result = evaluate_vs_random(
                model,
                games=args.eval_games,
                seed=seed + t,  # ä½¿ç”¨ä¸åŒçš„ç§å­
                depth=args.eval_depth,
                width=args.eval_width
            )
            current_winrate = eval_result["winrate"]
            
            logging.info(f"[stop-check] games={t}, winrate={current_winrate:.3f}, target={stop_winrate}")
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if current_winrate >= stop_winrate:
                logging.info(f"[stop] ğŸ¯ è¾¾åˆ°ç›®æ ‡èƒœç‡ {current_winrate:.3f} >= {stop_winrate}, åœæ­¢è®­ç»ƒ!")
                
                # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                try:
                    model.save(str(save_latest))
                    logging.info(f"[ckpt] æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ -> {save_latest}")
                except Exception as e:
                    logging.exception(f"[ckpt] æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
                
                return  # æå‰ç»“æŸè®­ç»ƒ
            # ========== æ–°å¢ï¼šèƒœç‡åœæ­¢æ£€æŸ¥ ==========

        if t % log_every == 0:
            elapsed = time.time() - t0
            n_batch = stat_w + stat_l + stat_d
            wr = stat_w / n_batch if n_batch else 0.0
            speed = log_every / max(1e-9, elapsed)
            logging.info(
                f"[train] games={t}/{episodes}  winrate={wr:.3f}  "
                f"w/l/d={stat_w}/{stat_l}/{stat_d}  speed={speed:.1f} games/s  eps={epsilon:.3f}"
            )
            stat_w = stat_l = stat_d = 0
            t0 = time.time()

        if eval_every > 0 and t % eval_every == 0:
            ev = evaluate_vs_random(model,
                                    games=args.eval_games,
                                    seed=seed,
                                    depth=args.eval_depth,
                                    width=args.eval_width)
            logging.info(
                f"[eval] games={ev['games']}  winrate={ev['winrate']:.3f} "
                f"95%CI=[{ev['ci_low']:.3f},{ev['ci_high']:.3f}]  "
                f"w/l/d={ev['wins']}/{ev['losses']}/{ev['draws']} "
                f"(eval_depth={args.eval_depth}, eval_width={args.eval_width})"
            )

        if ckpt_every > 0 and t % ckpt_every == 0:
            ckpt_path = Path(ckpt_dir) / f"model_ep{t}.cpt.npy"
            try:
                model.save(str(ckpt_path))
                model.save(str(latest_path))
                logging.info(f"[ckpt] saved -> {ckpt_path} (and updated {latest_path.name})")
            except Exception as e:
                logging.exception(f"[ckpt] save failed: {e}")

    try:
        model.save(str(latest_path))
        logging.info(f"[ckpt] final save -> {latest_path}")
    except Exception as e:
        logging.exception(f"[ckpt] final save failed: {e}")

# --------------------------- é‡‡é›†ï¼šå†™åˆ†ç‰‡ ---------------------------

class ShardWriter:
    def __init__(self, out_dir: str, max_rows: int):
        self.dir = Path(out_dir); self.dir.mkdir(parents=True, exist_ok=True)
        self.max_rows = int(max_rows)
        self.shard_idx = 0
        self.total = 0
        self.buf: Dict[str, list] = {
            "s": [], "a": [], "s_next": [], "r": [], "d": [],
            "legal_s": [], "legal_s_next": [],
        }

    def add(self, s, a, s_next, r, d, legal_s, legal_s_next):
        self.buf["s"].append(s)
        self.buf["a"].append(np.int16(a))
        self.buf["s_next"].append(s_next)
        self.buf["r"].append(np.float32(r))
        self.buf["d"].append(np.uint8(d))
        self.buf["legal_s"].append(legal_s)
        self.buf["legal_s_next"].append(legal_s_next)

    def flush(self):
        if len(self.buf["a"]) == 0:
            return
        n = len(self.buf["a"])
        out = {}
        for k, vs in self.buf.items():
            v0 = vs[0]
            if isinstance(v0, np.ndarray):
                out[k] = np.stack(vs)
            else:
                out[k] = np.asarray(vs)
        path = self.dir / f"shard_{self.shard_idx:05d}.npz"
        np.savez_compressed(path, **out)
        self.total += n
        logging.info(f"[collect] wrote {n} rows -> {path.name} (total={self.total})")
        self.shard_idx += 1
        for k in self.buf.keys():
            self.buf[k].clear()

# --------------------------- é‡‡é›†ä¸»å¾ªç¯ï¼ˆå« MC å›å¡«ï¼‰ ---------------------------

def collect_selfplay(episodes, model, depth, width, out_dir, max_rows, write_legal=True,
                     opening_random_moves=0, collect_eps=0.0,
                     stop_dup_rate=0.95, stop_min_seen=50_000,
                     log_every_games=50,
                     metrics_file="", metrics_every_games=50,
                     mc_reward=False, mc_mode="current",
                     selection_temperature=0.3):
    """
    åªé‡‡é›†ï¼Œä¸è®­ç»ƒã€‚ä¸¥æ ¼å”¯ä¸€ï¼š(canonical(s),a)ã€‚æ¯å±€ç»“æŸåå¯é€‰ MC å¥–åŠ±å›å¡«ã€‚
    """
    logging.info(f"[mode] collect-only -> {out_dir}  (depth={depth}, width={width})")
    writer = ShardWriter(out_dir, max_rows)

    rng = random.Random(2025)
    b = Board()

    keys_seen = set()       # å…¨å±€å”¯ä¸€é”®
    shard_keys = set()      # åˆ†ç‰‡å†…å”¯ä¸€é”®
    rows_in_shard = 0

    # æŒ‡æ ‡
    seen_total = 0
    unique_total = 0
    games_done = 0
    t0 = time.time()

    def cur_dup_rate():
        return 0.0 if seen_total == 0 else 1.0 - (unique_total / seen_total)

    # æ¯å±€ç¼“å†²ï¼ˆç”¨äº MC å›å¡«ï¼‰
    ep_buf: List[dict] = []

    while games_done < episodes:
        b.init_board()
        p = Board.BLACK
        black_bot = Bot(model, depth, width, Board.BLACK)
        white_bot = Bot(model, depth, width, Board.WHITE)

        # å¼€å±€éšæœºè‹¥å¹²æ­¥ï¼ˆä¸å†™æ•°æ®ï¼Œåªæ‰°åŠ¨èµ·ç‚¹ï¼‰
        rnd_moves = int(max(0, opening_random_moves))
        for _ in range(rnd_moves):
            if b.is_terminal_state():
                break
            opts = b.feasible_pos(p)
            if not opts:
                p = Board.opponent(p)
                continue
            i, j = rng.choice(opts)
            b.flip(i, j, p)
            p = Board.opponent(p)

        # è‡ªå¯¹å¼ˆç›´åˆ°ç»ˆå±€ï¼ŒæŒ‰â€œé‡‡é›† Îµâ€åš Îµ-greedyï¼ˆå¯¹ Bot çš„å»ºè®®ï¼‰
        while not b.is_terminal_state():
            options = b.feasible_pos(p)
            if len(options) == 0:
                p = Board.opponent(p)
                continue

            # Bot çš„å»ºè®®ï¼ˆè´ªå¿ƒåŠ¨ä½œï¼‰
            if p == Board.BLACK:
                _, (greed_i, greed_j) = black_bot._play(b)
            else:
                _, (greed_i, greed_j) = white_bot._play(b)

            # è®¡ç®—å€¼åˆ—è¡¨ï¼ˆç”¨ä¸ä¸Š vï¼Œä»…ä¸ºäº†å’Œ util.epsilon_greedy çš„æ¥å£ä¿æŒä¸€è‡´ï¼‰
            vals = []
            for (ii, jj) in options:
                with b.flip2(ii, jj, p):
                    if b.is_terminal_state():
                        vals.append(b.score(Board.BLACK) - b.score(Board.WHITE))
                    else:
                        vals.append(model(b))
            
            # # é‡‡é›† Îµï¼šä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ epsilonï¼ˆä¸è®­ç»ƒ Îµ æ— å…³ï¼‰
            # (a0, a1), _ = epsilon_greedy(collect_eps, options, vals, (greed_i, greed_j), 0.0)
            
            # === ä¿®æ”¹å¼€å§‹ï¼šæ›¿æ¢Îµ-greedyä¸ºä»·å€¼æ¦‚ç‡é€‰æ‹© ===
            if len(options) > 0:
                # å°†ä¼°å€¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                vals_array = np.array(vals)
    
            # å¤„ç†æç«¯å€¼ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
            vals_array = np.clip(vals_array, -10, 10)
    
            # åº”ç”¨softmaxï¼ˆæ¸©åº¦å‚æ•°=selection_temperatureï¼‰
            temperature = max(0.01,selection_temperature)
            exp_vals = np.exp(vals_array / temperature)
            probabilities = exp_vals / np.sum(exp_vals)
    
            # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·åŠ¨ä½œ
            chosen_idx = np.random.choice(len(options), p=probabilities)
            a0, a1 = options[chosen_idx]
            selected_val = vals_array[chosen_idx]
            # === ä¿®æ”¹ç»“æŸ ===

            # è®°å½•è½¬ç§»ï¼ˆåœ¨ flip å‰åšç¼–ç ï¼‰
            s_3ch = board_to_3ch_uint8(b, p)
            s_can = canonicalize_3ch(s_3ch)
            legal_s = legal_mask_from_board(b, p)

            # è½å­
            b.flip(a0, a1, p)
            p_next = Board.opponent(p)

            # ç»ˆå±€åˆ¤æ–­ + å¥–åŠ±ï¼ˆå³æ—¶å¥–åŠ±ï¼šä»…ç»ˆå±€ Â±1ï¼Œå¦åˆ™ 0ï¼‰
            done_now = b.is_terminal_state()
            if done_now:
                sb, sw = b.score(Board.BLACK), b.score(Board.WHITE)
                # å¯¹â€œå½“å‰è¿™æ­¥æ‰§æ‰‹â€çš„å³æ—¶å¥–åŠ±
                reward = 1.0 if (sb > sw and p == Board.BLACK) or (sw > sb and p == Board.WHITE) else \
                         (-1.0 if (sb < sw and p == Board.BLACK) or (sw < sb and p == Board.WHITE) else 0.0)
            else:
                reward = 0.0

            # ä¸‹ä¸€ä¸ªçŠ¶æ€ç¼–ç ï¼ˆä»ä¸‹ä¸€æ­¥æ‰§æ‰‹è§†è§’ï¼‰
            s_next_3ch = board_to_3ch_uint8(b, p_next)
            s_next_can = canonicalize_3ch(s_next_3ch)
            legal_next = legal_mask_from_board(b, p_next)
            a_idx = ij_to_a(a0, a1)

            # ä¸¥æ ¼å”¯ä¸€é”®ï¼ˆå…¨å±€ï¼‰
            key = hash64_sa(s_can, a_idx)
            seen_total += 1
            if key not in keys_seen:
                keys_seen.add(key)
                rec = dict(
                    s=s_can, a=a_idx, s_next=s_next_can,
                    r=np.float32(reward), d=np.uint8(1 if done_now else 0),
                    legal_s=legal_s if write_legal else np.zeros((64,), np.uint8),
                    legal_s_next=legal_next if write_legal else np.zeros((64,), np.uint8),
                )
                ep_buf.append(rec)

            # åˆ‡æ¢æ‰§æ‰‹æ–¹
            p = p_next

        # ======== ä¸€å±€ç»“æŸï¼šMC å›å¡« + å†™ç›˜ ========
        # ç»ˆå±€èƒœè´Ÿï¼ˆä»é»‘æ–¹å¾—åˆ†åˆ¤æ–­ï¼›ä»…ç”¨äº uniform æ¨¡å¼ï¼‰
        sb, sw = b.score(Board.BLACK), b.score(Board.WHITE)
        res_black = 1 if sb > sw else (-1 if sb < sw else 0)

        if mc_reward and len(ep_buf) > 0:
            if mc_mode == "current":
                # ä»¥â€œç»ˆå±€é‚£æ­¥çš„å³æ—¶å¥–åŠ±ç¬¦å·â€ä¸ºå°¾éƒ¨æ ‡ç­¾ï¼Œå‘å‰é€æ­¥äº¤æ›¿å–å
                r_end = float(ep_buf[-1]["r"])
                if   r_end >  0.5: z_tail = 1
                elif r_end < -0.5: z_tail = -1
                else:              z_tail = 0
                for j in range(len(ep_buf)-1, -1, -1):
                    ep_buf[j]["r"] = np.float32(z_tail)
                    z_tail = -z_tail
            elif mc_mode == "uniform":
                z_uni = 1 if res_black > 0 else (-1 if res_black < 0 else 0)
                for rec in ep_buf:
                    rec["r"] = np.float32(z_uni)

        # æŠŠæœ¬å±€ç¼“å­˜å†™å…¥ï¼ˆåˆ†ç‰‡å†…å†åšä¸€æ¬¡å»é‡ï¼‰
        for rec in ep_buf:
            k = hash64_sa(rec["s"], rec["a"])
            if k in shard_keys:
                continue
            writer.add(
                s=rec["s"], a=rec["a"], s_next=rec["s_next"],
                r=rec["r"], d=rec["d"],
                legal_s=rec["legal_s"], legal_s_next=rec["legal_s_next"]
            )
            unique_total += 1
            shard_keys.add(k)
            rows_in_shard += 1
            if rows_in_shard >= writer.max_rows:
                writer.flush()
                logging.info(
                    f"[collect] games={games_done}/{episodes}  "
                    f"buffered~{rows_in_shard}  flushed_total={writer.total}  "
                    f"seen_total={seen_total} unique_total={unique_total} dup_rate={cur_dup_rate():.3f}  "
                    f"speed={(log_every_games / max(1e-9, (time.time()-t0))):.2f} g/s"
                )
                shard_keys.clear()
                rows_in_shard = 0
                t0 = time.time()

        ep_buf.clear()
        games_done += 1

        # æ—¥å¿—å¿ƒè·³
        if games_done % log_every_games == 0:
            elapsed = time.time() - t0
            speed = log_every_games / max(1e-9, elapsed)
            logging.info(
                f"[collect] games={games_done}/{episodes}  "
                f"buffered~{rows_in_shard}  flushed_total={writer.total}  "
                f"seen_total={seen_total} unique_total={unique_total} dup_rate={cur_dup_rate():.3f}  "
                f"speed={speed:.2f} g/s"
            )
            t0 = time.time()

        # æ—©åœï¼šå…¨å±€é‡å¤ç‡ / æœ€å° seen
        if (seen_total >= stop_min_seen) and (cur_dup_rate() >= stop_dup_rate):
            logging.info(f"[collect] stop by dup_rate: seen_total={seen_total} unique_total={unique_total} dup_rate={cur_dup_rate():.3f}")
            break

    # æ”¶å°¾ flush
    if rows_in_shard > 0:
        writer.flush()

# --------------------------- å…¥å£ ---------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--episodes", type=int, default=700_000, help="æ€»è‡ªåšå¼ˆ/é‡‡é›†å±€æ•°ä¸Šé™")
    parser.add_argument("--epsilon", type=float, default=0.05, help="è®­ç»ƒ Îµ-greedy")
    parser.add_argument("--log-every", type=int, default=100, help="è®­ç»ƒæ¯å¤šå°‘å±€æ‰“å°ä¸€æ¬¡")
    parser.add_argument("--eval-every", type=int, default=10_000, help="è®­ç»ƒæ¯å¤šå°‘å±€è¯„æµ‹ï¼ˆ0=å…³é—­ï¼‰")
    parser.add_argument("--ckpt-every", type=int, default=10_000, help="è®­ç»ƒæ¯å¤šå°‘å±€ä¿å­˜ï¼ˆ0=å…³é—­ï¼‰")
    parser.add_argument("--ckpt-dir", type=str, default="model", help="checkpoint ä¿å­˜ç›®å½•")
    parser.add_argument("--latest", type=str, default="model/model.cpt.npy", help="æœ€æ–°å¿«ç…§æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--load", type=str, default="model/model.cpt.npy.6", help="å¯åŠ¨æ—¶åŠ è½½çš„æƒé‡ï¼ˆå¯ç•™ç©ºï¼‰")
    parser.add_argument("--lr", type=float, default=0.001, help="ModelScorer å­¦ä¹ ç‡")
    parser.add_argument("--gamma", type=float, default=0.01, help="TD å¹³æ»‘ç³»æ•°ï¼ˆæŒ‰ä½ å®ç°è¯­ä¹‰ï¼‰")
    parser.add_argument("--seed", type=int, default=2025, help="éšæœºç§å­")
    parser.add_argument("--logfile", type=str, default="tdl.log", help="æ—¥å¿—æ–‡ä»¶")

    # è®­ç»ƒ/è¯„æµ‹æœç´¢å¼ºåº¦åˆ†ç¦»
    parser.add_argument("--train-depth", type=int, default=1, help="è®­ç»ƒ Bot æ·±åº¦")
    parser.add_argument("--train-width", type=int, default=2, help="è®­ç»ƒ Bot å®½åº¦")
    parser.add_argument("--eval-depth", type=int, default=3, help="è¯„æµ‹ Bot æ·±åº¦")
    parser.add_argument("--eval-width", type=int, default=6, help="è¯„æµ‹ Bot å®½åº¦")
    parser.add_argument("--eval-games", type=int, default=200, help="è¯„æµ‹å¯¹å±€æ•°")

    # é‡‡é›†æ¨¡å¼å¼€å…³
    parser.add_argument("--collect-out", type=str, default="", help="éç©ºåˆ™è¿›å…¥é‡‡é›†æ¨¡å¼ï¼Œè¾“å‡ºç›®å½•")
    parser.add_argument("--collect-max-rows", type=int, default=200_000, help="æ¯ä¸ªåˆ†ç‰‡çš„è¡Œæ•°ä¸Šé™")
    parser.add_argument("--opening-random-moves", type=int, default=0, help="é‡‡é›†æ—¶å¼€å±€éšæœºæ­¥æ•°")
    parser.add_argument("--collect-eps", type=float, default=0.0, help="é‡‡é›†æ—¶ Îµ-greedyï¼ˆä¸è®­ç»ƒ Îµ æ— å…³ï¼‰")
    parser.add_argument("--stop-dup-rate", type=float, default=0.95, help="é‡‡é›†æ—©åœï¼šå…¨å±€é‡å¤ç‡é˜ˆå€¼")
    parser.add_argument("--stop-min-seen", type=int, default=50_000, help="é‡‡é›†æ—©åœï¼šæœ€å°è§‚æµ‹æ¡æ•°")

    # MC å›å¡«
    parser.add_argument("--mc-reward", action="store_true",
                        help="é‡‡é›†æ—¶å°†æœ¬å±€æ‰€æœ‰æ­¥éª¤çš„å¥–åŠ±æ›¿æ¢ä¸º Monte-Carlo ç»ˆå±€æ ‡ç­¾")
    parser.add_argument("--mc-mode", type=str, default="current",
                        choices=["current", "uniform"],
                        help="MC æ ‡ç­¾æ¨¡å¼ï¼šcurrent=å½“å‰æ‰§æ‰‹äº¤æ›¿å–ç¬¦å·ï¼›uniform=æ•´å±€ç»Ÿä¸€æ ‡ç­¾")
    
    # æ–°å¢èƒœç‡åœæ­¢å‚æ•°
    parser.add_argument("--stop-winrate", type=float, default=0.95, 
                       help="å½“å¯¹éšæœºæ¨¡å‹çš„èƒœç‡è¾¾åˆ°æ­¤å€¼æ—¶åœæ­¢è®­ç»ƒ")
    
    # æ–°å¢ä»·å€¼æ¦‚ç‡é€‰æ‹©çš„æ¸©åº¦å‚æ•°ï¼ˆé‡‡é›†æ¨¡å¼ä¸“ç”¨ï¼‰
    parser.add_argument("--selection-temperature", type=float, default=0.3,
                        help="ä»·å€¼æ¦‚ç‡é€‰æ‹©çš„æ¸©åº¦å‚æ•°ï¼ˆé‡‡é›†æ¨¡å¼ä¸“ç”¨ï¼‰")

    args = parser.parse_args()

    os.environ["PYTHONUNBUFFERED"] = "1"
    setup_logging(args.logfile, verbose=True)

    # æ¨¡å‹
    model = ModelScorer(learning_rate=args.lr, gamma=args.gamma)
    if args.load and Path(args.load).exists():
        try:
            model.load(args.load)
            logging.info(f"[init] loaded weights from {args.load}")
        except Exception as e:
            logging.exception(f"[init] load failed: {e}")
    else:
        logging.info("[init] start from fresh weights")

    # é‡‡é›†æ¨¡å¼
    if args.collect_out:
        collect_selfplay(
            episodes=args.episodes,
            model=model,
            depth=args.train_depth,
            width=args.train_width,
            out_dir=args.collect_out,
            max_rows=args.collect_max_rows,
            write_legal=True,
            opening_random_moves=args.opening_random_moves,
            collect_eps=args.collect_eps,
            stop_dup_rate=args.stop_dup_rate,
            stop_min_seen=args.stop_min_seen,
            log_every_games=50,
            metrics_file="",
            metrics_every_games=50,
            mc_reward=args.mc_reward,
            mc_mode=args.mc_mode,
            selection_temperature=args.selection_temperature,
        )
        # æ”¶å°¾
        try:
            model.save(args.latest)
            logging.info(f"[ckpt] final save -> {args.latest}")
        except Exception as e:
            logging.exception(f"[ckpt] final save failed: {e}")
        return

    # è®­ç»ƒæ¨¡å¼
    self_play(episodes=args.episodes,
              model=model,
              epsilon=args.epsilon,
              log_every=args.log_every,
              eval_every=args.eval_every,
              ckpt_every=args.ckpt_every,
              ckpt_dir=args.ckpt_dir,
              save_latest=args.latest,
              seed=args.seed,
              train_depth=args.train_depth,
              train_width=args.train_width,
              args=args,
              stop_winrate=args.stop_winrate)

if __name__ == '__main__':
    main()
